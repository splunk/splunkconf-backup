[settings]

# this configuration file is used to customize splunkconf-backup

# 20170606 changed default backups directory to be in splunk_home/var as more chance to have room in it
# 20180619 split backup parameters in distinct conf file to isolate backup parameters and move backup script to subdirectory
# 20180619 add distinct local and remote purge settings, add parameters for remote backup (nas, s3, ...)
# 20180619 move content to script under default in order to have a clean view on modified settings here
# 20220326 remove var that contain path to backup as this is now dynamic depending on rel mode
# 20240630 with 1.9+, change the way settings are loaded and make it more splunky with a settings stanza

# this file is being included in script (bash)
# please override only the local in local as usual 


# splunkconf backup
#

# DEBUG 

# set DEBUG to 1 in conf file (or set via tag splunkbackupdebug=1)
#DEBUG=1

# backup type selection 
# 1 = targeted etc -> use a list of directories and files to backup
# 2 = full etc   (bigger)
# default to targeted etc
BACKUPTYPE=1

# LOCAL AND REMOTE BACKUP options

# LOCAL options
# NOT used this is enforced ! DOLOCALBACKUP=1
# type : 1 = date versioned backup (preferred for local),2 = only one backup file with instance name in it (dangerous, we need other feature to do versioning like filesystem (btrfs) , classic backup on top, ...  3 = only one backup, no instance name in it (tehy are still sorted by instance directory, may be easier for automatic reuse by install scripts)
LOCALTYPE=1
# where to store local backups
# depending on partitions
# splunk user should be able to write to this directory
LOCALBACKUPDIR="${SPLUNK_HOME}/var/backups"
# Reserve enough space or backups will fail ! IMPORTANT
# see below for check on min free space

# REMOTE options
# enabled by default , try to get s3 bucket info from ec2 tags or just do nothing
DOREMOTEBACKUP=1
# exemple, please tune
# PLEASE USE A SUBDIRECTORY ON THE REMOTE LOCATION SO THAT THE DIRECTORY CHECK WILL FAIL IN CASE THE REMOTE STORAGE IS NOT THERE (we don't want to write locally if not mouted for example)
#REMOTEBACKUPDIR="/mnt/remotenas/backup"
REMOTEBACKUPDIR="s3://pleaseconfigureec2tagsorsetdirectlythes3bucketforbackupshere-s3splunkbackup/splunkconf-backup"
# type 1 = nas (use cp), 2 = S3 (use aws s3 cp), 3= remote nas via scp, 4- rsync over ssh
REMOTETECHNO=2
# type : 0=auto 1 = date versioned backup (preferred for local),2 = only one backup file (dangerous, we need other feature to do versioning like filesystem (btrfs) , classic backup on top, ... 3 = only one backup, no instance name in it (they are still sorted by instance directory, may be easier for automatic reuse by install scripts)
# auto -> S3 =0 (because s3 can store multiple versions of same file), NAS=1
REMOTETYPE=0

# see https://docs.aws.amazon.com/AmazonS3/latest/API/API_HeadObject.html for valid values
# see https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html for class descriptions
#REMOTES3STORAGECLASS="STANDARD_IA"
REMOTES3STORAGECLASS="auto"

# STD as we keep it less than 30 days
#REMOTES3STORAGECLASSHOURLY="STANDARD"
REMOTES3STORAGECLASSHOURLY="auto"

#REMOTES3STORAGECLASSDAILY="STANDARD-IA"
REMOTES3STORAGECLASSDAILY="auto"

#REMOTES3STORAGECLASSWEEKLY="STANDARD-IA"
REMOTES3STORAGECLASSWEEKLY="auto"

#REMOTES3STORAGECLASSMONTHLY="STANDARD-IA"
REMOTES3STORAGECLASSMONTHLY="auto"

# 0 = auto (1 at the moment)
# 1 = use aws s3 cp (no support for tags), tag command done in a second step
# 2 = use aws s3-apui copy-object (support tags) (require iam correctly set)

AWSCOPYMODE=0

# by default we will tag objects send to s3, set this to 0 to disable it 
REMOTEOBJECTSTORETAGS3="1"

# you dont need to set this unless doing backup to object store on prem 
# it is better to use tag on instance metadata so the app config is the same between different env
REMOTEOBJECTSTOREBUCKET="auto"
# this is the prefix
REMOTEOBJECTSTOREPREFIX="splunkconf-backup"

# for on prem s3, specify the endpoint url as http(s)://mys3endpoint , otherwise let the auto setting
# this will add  --endpoint-url $REMOTES3ENDPOINTURL to aws commands
REMOTES3ENDPOINTURL="auto"

# for REMOTETECHNO=3   (rcp)
#REMOTEBACKUPDIR="/opt/splunk/var/remotebackups"
RCPHOST="notset"
RCPREMOTEUSER="splunk"


# for REMOTECHNO=4 (rsync over ssh)
RSYNCHOST="notset"
RSYNCREMOTEUSER="splunk"
# 0 = do nothing (the remote splunk is not enabled so splunkconfbackup-purge will not run ! ) DANGEROUS
# 1 = enable remote delete via rsync (to avoid filling remote disk which would defeat rsync) -> fine but in case of split brain situation (2 instances active), that could backfire (ie the instance with the less current content would overwirte backups on the other part)
# 2 = launch remote splunkconf-purgebackup script -> this may be more resistent in case of split brain (as long as enough space has been allocated to backups)
RSYNCREMOTEDELETE=2
# ask to remote host to stop splunk service as we are not supposed to have the same splunk enabled twice
RSYNCDISABLEREMOTE=1

# enable autorestore via rsync over ssh
RSYNCAUTORESTORE=1

# WHAT TO BACKUP
# this can be used for : rollback, doing diff , moving conf to new server, ....
# this is not meant to backup files binaries, var , index, ....
# you should backup the backup ie not just keep it local to avoid the server has crashed and the script deleting the backups....
# set to activate backups
BACKUP=1
# set to backup kvstore
BACKUPKV=1
# set to backup scheduler state, modinput (hf but also useful on sh for dm and throttling) 
BACKUPSTATE=1
# set to backup scripts
BACKUPSCRIPTS=1



#minfreespace

# 5000000 = 5G
# more than splunk daemon limit so that we hopefully stop doing backups before splunk reach his own limit (5G by default)
# obviously if you have done the right sizing for backups, the other constraint should win first as we could be in a situation of disk starvation preventing any launch of backup for a while
MINFREESPACE=6000000


##### LOCAL


# number of days after which we completely remove backups
LOCALBACKUPRETENTIONDAYS=20
# number of days after which we remove most backups to free up space (this should be under the first parameter)
# idea is that we may have a high frequency backup for recent ones then we only keep a few (just in case, we want to go back to a old situation)
# NOT YET IMPLEMENTED
LOCALBACKUPRETENTIONDAYSPARTIAL=180
# KV
LOCALBACKUPKVRETENTIONDAYS=20
# scripts
LOCALBACKUPSCRIPTSRETENTIONDAYS=100
# modinput/state
LOCALBACKUPMODINPUTRETENTIONDAYS=7
LOCALBACKUPSTATERETENTIONDAYS=7

LOCALMAXSIZE=7200000000 
#2G
# uncomment to test purge 
#LOCALMAXSIZE=2000000000 #2G

##### REMOTE 
# number of days after which we completely remove backups
REMOTEBACKUPRETENTIONDAYS=180
# number of days after which we remove most backups to free up space (this should be under the first parameter)
# idea is that we may have a high frequency backup for recent ones then we only keep a few (just in case, we want to go back to a old situation)
# NOT YET IMPLEMENTED
REMOTEBACKUPRETENTIONDAYSPARTIAL=180
# KV
REMOTEBACKUPKVRETENTIONDAYS=60
# scripts
REMOTEBACKUPSCRIPTSRETENTIONDAYS=200
# modinput
REMOTEBACKUPMODINPUTRETENTIONDAYS=7
REMOTEBACKUPSTATERETENTIONDAYS=7

REMOTEMAXSIZE=100000000000 #100G




